{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import traceback\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import common\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from common import Network1, Network2, Network3, Network4, Network5, Network6, Network24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.INFO)\n",
    "logging.debug('A debug message!')\n",
    "\n",
    "import getopt\n",
    "dataset_idx = -1\n",
    "try:\n",
    "    dataset_idx = int(sys.argv[sys.argv.index(\"-d\") + 1])\n",
    "    seed = int(sys.argv[sys.argv.index(\"-s\") + 1])\n",
    "    print(\"Dataset index: \", dataset_idx)\n",
    "    print(\"Seed: \", seed)\n",
    "    torch.manual_seed(seed)\n",
    "except ValueError:\n",
    "    print(\"Invalid value for dataset index or seed. Both must be integers.\")\n",
    "    sys.exit(1)\n",
    "except IndexError:\n",
    "    print(\"Please provide both dataset index with flag -d and seed with flag -s\")\n",
    "    print(\"Usage: python script.py -d <dataset_index> -s <seed>\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# create folder data/models_and_boundaries/ if it does not exist\n",
    "Path(\"data/models_and_boundaries/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create folder data/experiments/ if it does not exist\n",
    "Path(\"data/experiments/\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the \"GeneratorInput.txt\" file\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.readlines()\n",
    "    return data\n",
    "\n",
    "raw_data = read_data(\"GeneratorInput.txt\")\n",
    "\n",
    "datasets = []\n",
    "y_idx = 0\n",
    "dataset = None\n",
    "for line in raw_data:\n",
    "    if len(line) <= 1:\n",
    "        datasets.append(dataset.copy())\n",
    "        y_idx = 0\n",
    "        dataset = []\n",
    "        continue\n",
    "    y_idx += 1\n",
    "    if dataset is None:\n",
    "        dataset = []\n",
    "    for x_idx,char in enumerate(line):\n",
    "        if char == '0' or char == '1':\n",
    "            dataset.append((y_idx,x_idx+1, int(char)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initiate_experiment(raw_dataset, model,batch_size=1):\n",
    "    LEARNING_RATE = 1\n",
    "\n",
    "    print(f\"batch size is {batch_size} and learning rate is {LEARNING_RATE}\")\n",
    "    \n",
    "    X = torch.tensor([list(sublist[:2]) for sublist in raw_dataset], dtype=torch.float32)\n",
    "    y = torch.tensor([item for sublist in raw_dataset for item in sublist[2:]], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    #scale the data to be between 0 and 1x\n",
    "    X = X / X.max()\n",
    "\n",
    "    # Create a PyTorch dataset and data loader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # divide parameters into groups: exponent, weight, bias\n",
    "    bias_params = [param for name, param in model.named_parameters() if \"bias\" in name]\n",
    "    weight_params = [param for name, param in model.named_parameters() if \"weight\" in name]\n",
    "    exponent_params = [param for name, param in model.named_parameters() if \"exponent\" in name]\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {\"params\": bias_params, \"lr\": LEARNING_RATE},\n",
    "        {\"params\": weight_params, \"lr\": LEARNING_RATE},\n",
    "        {\"params\": exponent_params, \"lr\": LEARNING_RATE}\n",
    "    ])\n",
    "\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    return model, X, y, dataloader, criterion, optimizer\n",
    "\n",
    "def train_model(model, X, y ,dataloader, criterion, optimizer, num_epochs,experiment_name,total_epochs=0):\n",
    "    # start measuring time\n",
    "    start = time.time()\n",
    "    total_epochs = 0\n",
    "\n",
    "    # Train the model\n",
    "    success = False\n",
    "    for epoch in range(num_epochs):\n",
    "        total_epochs += 1\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # calculate accuracy for all training data\n",
    "        y_pred = model(X)\n",
    "        y_pred = torch.round(y_pred)\n",
    "        correct = (y_pred == y).sum().item()\n",
    "        accuracy = correct / len(y)\n",
    "\n",
    "        if accuracy == 1:\n",
    "            print(f\"HOORAY, Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}\")\n",
    "            success = True\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}\")\n",
    "            \n",
    "    # end measuring time\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    print(f\"Time taken: {time_taken:.2f}s\")\n",
    "\n",
    "    if not success:\n",
    "        experiment_name = f\"{experiment_name}_failed\"\n",
    "    \n",
    "    common.plot_decision_boundary(model, X, y, save_file_name=f\"data/models_and_boundaries/{experiment_name}.png\") \n",
    "\n",
    "    # save model to disk\n",
    "    torch.save(model.state_dict(), f\"data/models_and_boundaries/{experiment_name}.pth\")\n",
    "        \n",
    "    print(f\"total epochs: {total_epochs}\")\n",
    "\n",
    "    # print all parameters of the model\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name} has shape {param.shape} and value {param}\")\n",
    "\n",
    "    # print model gradients\n",
    "    for name, param in model.named_parameters(): \n",
    "        print(f\"{name} has gradient {param.grad}\")\n",
    "        \n",
    "\n",
    "    return success, total_epochs, time_taken\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [Network1, Network2, Network3, Network4, Network5, Network6, Network24]\n",
    "\n",
    "experiments = []\n",
    "\n",
    "EXPERIMENT_FILE = f\"data/experiments/experiments_{dataset_idx+1}.json\"\n",
    "# check if the experiment file exists\n",
    "try:\n",
    "    with open(EXPERIMENT_FILE, 'r') as file:\n",
    "        experiments = json.load(file)\n",
    "        print(\"loaded experiments from file\")\n",
    "except FileNotFoundError:\n",
    "    for model_idx,network in enumerate(all_models):\n",
    "        experiment_name = f\"dataset_{dataset_idx+1}_network_{network.__name__}\"\n",
    "        experiments.append({\"name\":experiment_name, \"model\":model_idx , \"done\":False , \"dataset_idx\": dataset_idx})\n",
    "    print(\"created new experiments\")\n",
    "print(experiments)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for experiment in experiments:\n",
    "        try:\n",
    "            print(f\"EXPERIMENT {experiment['name']} started\")\n",
    "            model = all_models[experiment['model']]()\n",
    "            if experiment['done']:\n",
    "                print(f\"EXPERIMENT {experiment['name']} already done\")\n",
    "                continue\n",
    "            model, X, y, dataloader, criterion, optimizer = initiate_experiment(datasets[experiment[\"dataset_idx\"]], model)\n",
    "            success, total_epochs,time_taken = train_model(model, X, y, dataloader, criterion, optimizer, 5000, experiment_name=experiment['name'])\n",
    "            if success:\n",
    "                experiment['done'] = True\n",
    "                experiment['total_epochs'] = total_epochs\n",
    "                experiment['time_taken'] = time_taken\n",
    "                print(f\"EXPERIMENT {experiment['name']} finished\")\n",
    "            else:\n",
    "                print(f\"EXPERIMENT {experiment['name']} max iterations reached\")\n",
    "            \n",
    "        except:\n",
    "            print(\"EXPERIMENT failed\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "\n",
    "print(f\"experiments status: {experiments}\")\n",
    "\n",
    "# save experiments as a json file\n",
    "\n",
    "with open(EXPERIMENT_FILE, \"w\") as f:\n",
    "    json.dump(experiments, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
